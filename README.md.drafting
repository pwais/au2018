# AU: Search for "Gold"

This repo is a project to search for a PAC-style relation between
training data and accuracy for convolutional neural networks.

Named after Aurie Ginsberg

## Setup

See [the `cluster` module](cluster/README.md) about setting up a cluster.



https://www.cs.cmu.edu/~tom/10701_sp11/recitations/Recitation_9.pdf

pytest au -s --runslow --runhighmem -k test_mnist_save_png


lolololololol https://github.com/databricks/spark-deep-learning/blob/973e9da5216f69763f44da37d6039d205ddd5ade/python/sparkdl/estimators/keras_image_file_estimator.py#L194



reverse ssh tunnel bastion 
https://blog.trackets.com/2014/05/17/ssh-tunnel-local-and-remote-port-forwarding-explained-with-examples.html 
https://www.youtube.com/watch?v=JmjqPpQdtW8





for today:
 * get inferences for mnist, mobilenet, and deeplab and/or maskrcnn working
 * get the above running using sparkdl
 * make a demo script / unit test
 * plug in alluxio cache/ dump activations



build a bridge.
 * try to repro the GAN paper thing wher we can sample from mnist layer and
    generate grad image via guided backprop
 * 


```
curl https://sdk.cloud.google.com | bash
```

```
gcloud auth login
gcloud config set project avian-augury-217704
gcloud components install docker-credential-gcr
gcloud auth configure-docker

--OR--
gcloud auth print-access-token | docker login -u oauth2accesstoken --password-stdin https://gcr.io

```


```
nvidia-docker run -d -it --name au1 -v `pwd`:/opt/au -v /:/outer_root --net=host -P --privileged tensorflow/tensorflow:1.10.1-gpu sleep infinity
docker exec -it -w /opt/au -e COLUMNS="`tput cols`" -e LINES="`tput lines`" au1 bash
```



```
gcloud ubuntu-minimal-1604-xenial-v20180814
cd external/kubespray
ansible-playbook -v --become -i kubespray/inventory/default/hosts.ini external/kubespray/cluster.yml

kubespray -- need to override for nvidia-docker in /etc/ docker default runtime
```




 * mnist, cifar 10?  bdd100k some vids, mscoco
 
 * mnist simple net
 * alexnet cifar 10
 * inception / maskrcnn on bdd100k, mscoco
 
 
 * write a tool to record activations.  take an activation and deconv to input image.
    what happens when we perturb activations a bit?
 
 * write a spark tool to take a model and collect all activations at scale
 * 


NB: gcsfuse does NOT do read cache :P :P :P 
but s3fs does! https://github.com/s3fs-fuse/s3fs-fuse/wiki/Google-Cloud-Storage

tool for phone demo? https://cordova.apache.org/

# TODO
 * docker image to serve as dev env.  try to set up GCR?
 * notebook or runbook for setting up a k8s cluster with gluster and alluxio and nvidia drivers
 * add some submodules
 * try to repro the deep segmentation thing
 * stand up mscoco and kitti and bdd tfrecords



nice latent model paper: http://people.csail.mit.edu/rosman/papers/iros-2018-variational.pdf

parzen window estimation of sample:
 * https://www.mit.edu/~andoni/LSH/  euclidean LSH
     * http://mlwiki.org/index.php/Euclidean_LSH
 * this will get you standard parzen window prob !
 * can can get Confidence bands.. tho these look bad in figure 4 of 4.6 Confidence Bands and the CLT
    * http://www.stat.cmu.edu/~larry/=sml/densityestimation.pdf


experiments:
 1) tool for measuring approx of full joint of network
      using (at first random) partitions.  so for
      each partition we measure the full joint (or some approx of it)
      and then assume partitions are independent.  can combine multiple
      partitionings to get a better estimate methinks? ideally 
        * these partitions cover neurons that fire together or are related via
             pool and conv ops, since those definitely dependent
        * ideally if we span layers, then careful about dependence /
             independence of upper layers on lower layers
     
   ** tool:
      * compute probability of an input image given activations
      * find partitions that are in a low-prob state and backtrace to
          image... wanna highlight them.  try to do this via
          saliency or filter gradient or something.
          
 2) parzen window accelerated using LSH.  we will use this to estimate large
      joint distributions between neurons using recordings of activations.  so:
   * record all activations for test set
   * for each new example, compute acivation, then compute p(act) using
       parzen windows on referenece set, accelerated using LSH
   * that gives you a point estimate of the relevant joint
   * should be able to use this for big 'blobs' of a layer, like 
       object-size chunks of the lower conv layers
   * we can get z-score using some complicated bootstrapping thing

 3) train a deconv network that goes from probability descriptor
      to image?  then can sample from descriptor to generate
      input images  http://people.csail.mit.edu/rosman/papers/iros-2018-variational.pdf
      AND should be able to bias sampling in ways that sample from low-prob
      regions.  this is mainly for debugging versus the GAN-near-edges thing?
      we could potentially use a GAN for this part using our activation
      sample as a prior   https://arxiv.org/pdf/1411.1784.pdf or pix2pix
      
  
deepmind prob gan paper
https://arxiv.org/pdf/1806.05034.pdf

bayes gan 
https://github.com/andrewgordonwilson/bayesgan
https://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/

pgao paper
http://people.csail.mit.edu/rosman/papers/iros-2018-variational.pdf


** parzen windows for high dim.  page 17 http://www.stat.cmu.edu/~larry/=sml/densityestimation.pdf

faster tsne:
herm https://github.com/DmitryUlyanov/Multicore-TSNE
https://github.com/saurfang/spark-tsne/tree/master/spark-tsne-core/src/main/scala/com/github/saurfang/spark/tsne/tree

mean shift?
http://dimacs.rutgers.edu/archive/Workshops/Depth/meer.pdf


http://ai-benchmark.com/ranking.html
https://github.com/PAIR-code/saliency
https://research.google.com/colaboratory/local-runtimes.html
https://openreview.net/forum?id=ryiAv2xAZ
https://web.eecs.umich.edu/~honglak/hl_publications.html
https://pair-code.github.io/saliency/
http://matthewalunbrown.com/mops/mops.html
https://github.com/clemenscorny/brisk/blob/master/LICENSE
https://arxiv.org/pdf/1312.6034.pdf
http://cmp.felk.cvut.cz/data/motorway/





do NOT use docker for mac due to host networking issue that has been known but unfixed since 2016 https://blog.bennycornelissen.nl/post/docker-for-mac-neat-fast-and-flawed/


https://github.com/Lasagne/Recipes/blob/master/examples/Saliency%20Maps%20and%20Guided%20Backpropagation.ipynb

https://github.com/conan7882/CNN-Visualization


hot stuff
exactly!  see edge-generated examples https://arxiv.org/pdf/1711.09325.pdf 
https://openreview.net/forum?id=ryiAv2xAZ
codes!!  https://github.com/alinlab/Confident_classifier 

also
https://github.com/ShiyuLiang/odin-pytorch
https://github.com/facebookresearch/odin <-- hmm CC license

can we use LSH to sample from feature map latent space?  scooped!
https://arxiv.org/pdf/1802.07444.pdf
 

https://arxiv.org/pdf/1610.02136.pdf ... better! out of distribution !!! nips 18   https://arxiv.org/pdf/1807.03888.pdf   and detecting novel https://arxiv.org/pdf/1804.00722.pdf   https://web.eecs.umich.edu/~honglak/hl_publications.html   https://arxiv.org/pdf/1711.09325.pdf
https://arxiv.org/pdf/1705.08664.pdf  could be very good metric, seem to use gaussian input to generate images tho using a network?

daylen https://arxiv.org/pdf/1701.02362.pdf 




# oops permissions problems with s3fs and gcloud :P   
# https://stackoverflow.com/questions/17544139/allowing-permission-using-s3fs-bucket-directory-for-other-users
# https://github.com/s3fs-fuse/s3fs-fuse/issues/169
* Connection #0 to host au2018gs.storage.googleapis.com left intact
[INF]       curl.cpp:RequestPerform(2062): HTTP response code 200
[INF]       cache.cpp:AddStat(356): add stat cache entry[path=/au2018/]
   NODEID: 2
   unique: 2, success, outsize: 144
unique: 3, opcode: GETXATTR (22), nodeid: 2, insize: 65, pid: 13768
   unique: 3, error: -38 (Function not implemented), outsize: 16



^C[INF] s3fs.cpp:s3fs_destroy(3432): destroy
root@default:/tmp/s3fs-fuse# /usr/local/bin/s3fs -d -d -f au2018gs /mnt/s3fs_gcs -o use_cache=/opt/au/s3fs_cache4 -o passwd_file=/etc/gcs-auth.txt -o url=https://storage.googleapis.com -o sigv2 -o nomultipart -o allow_other,uid=`id -u`,umask=0077,mp_umask=0077 -o curldbg -o use_rrs
#  apt-get install -y s3fs

