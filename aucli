#!/usr/bin/env python
# vim: tabstop=2 shiftwidth=2 expandtab

DESC = """
aucli is a tool for boostrapping the AU project.  This tool has limited
dependencies (e.g. just python) so that it can run w/out the source tree.

## Example Workflow
Drop into a dockerized development shell:
  aucli --shell

Now develop!  Run tests:
  pytest au -s --runslow -k test_mnist_save_png

## (Re)-build the dockerized environment image
Outside of the dockerized environment, use:
  aucli --build-env

"""

import os
import subprocess
import sys

AU_ROOT = os.environ.get('AU_ROOT', os.path.dirname(os.path.abspath(__file__)))
DOCKER_IMAGE = os.environ.get('AU_DOCKER_IMAGE', 'au2018/env:v1')
CONTAINER_NAME = os.environ.get('AU_CONTAINER_NAME', 'au')

## Logging
import logging
LOG_FORMAT = "%(asctime)s\t%(name)-4s %(process)d : %(message)s"
log = logging.getLogger("au")
log.setLevel(logging.INFO)
console_handler = logging.StreamHandler(sys.stderr)
console_handler.setFormatter(logging.Formatter(LOG_FORMAT))
log.addHandler(console_handler)

## Utils

def run_cmd(cmd):
  cmd = cmd.replace('\n', '').strip()
  log.info("Running %s ..." % cmd)
  subprocess.check_call(cmd, shell=True)
  log.info("... done with %s " % cmd)

## Env

class DockerEnv(object):
  
  @classmethod
  def build(cls):
    run_cmd(
      'docker build -t ' + DOCKER_IMAGE + ' -f ' + AU_ROOT + '/docker/Dockerfile ' + AU_ROOT)
    run_cmd(
      'docker push ' + DOCKER_IMAGE)

  @classmethod
  def shell(cls):
    have_nvidia_docker = False
    try:
      run_cmd('nvidia-docker --help > /dev/null')
      have_nvidia_docker = True
    except Exception:
      log.info("Not using nvidia-docker")
    
    env_arg = ''
    if os.path.exists('my.env'):
      env_arg = '--env-file my.env'
    
    # Persist ivy2 / spark jar cache on host
    IVY2_PERSISTED_DIR = '/tmp/au_ivy2'
    run_cmd('mkdir -p %s' % IVY2_PERSISTED_DIR)
    
    docker = 'nvidia-docker' if have_nvidia_docker else 'docker'
    CMD = """
      {docker} run
        --name {container_name}
        -d -it -P
        --net=host
        -v `pwd`:/opt/au:z
        -v {ivy2_persisted_cache}:/root/.ivy2:z
        -v /:/outer_root:z
        -w /opt/au
        {env_arg}
          {docker_image} sleep infinity || docker start {container_name} || true
    """.format(
          docker=docker,
          container_name=CONTAINER_NAME,
          docker_image=DOCKER_IMAGE,
          env_arg=env_arg,
          ivy2_persisted_cache=IVY2_PERSISTED_DIR)
    run_cmd(CMD)
    
    # https://github.com/moby/moby/issues/33794#issuecomment-323003975
    #-c "export COLUMNS=`tput cols`; export LINES=`tput lines`; exec bash"'
    EXEC_CMD = 'docker exec -it %s bash' % CONTAINER_NAME
    os.execvp("docker", EXEC_CMD.split(' '))

  @classmethod
  def rm_shell(cls):
    try:
      run_cmd('docker rm -f %s' % CONTAINER_NAME)
    except Exception:
      pass
    log.info("Removed container %s" % CONTAINER_NAME)

## External Projects

def setup_tf_models():
  TF_MODELS_PATH = os.path.join(AU_ROOT, 'external/tensorflow_models/research')
  if not os.path.exists(TF_MODELS_PATH):
    log.warn("Skipping TF Models setup, can't find %s" % TF_MODELS_PATH)
  
  # Do https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md#manual-protobuf-compiler-installation-and-usage
  run_cmd("""
    ls -lhat /opt/au/external/tensorflow_models/research/object_detection/protos/train_pb2.py ||
      ( mkdir -p /opt/protobuf3 ;
        cd /opt/protobuf3 &&
        wget -O protobuf.zip https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip &&
        unzip protobuf.zip &&
        cd - &&
        cd /opt/au/external/tensorflow_models/research &&
        /opt/protobuf3/bin/protoc object_detection/protos/*.proto --python_out=. &&
        python object_detection/builders/model_builder_test.py )
    """ % TF_MODELS_PATH)
  
  #run_cmd("""
  #  ls -lhat /tmp/raw-data &&
  #  bash object_detection/dataset_tools/download_and_preprocess_mscoco.sh /tmp
  #""")



## Services

def test_spark():  
  log.info("Finding spark ...")
  import findspark
  findspark.init()
  log.info("... found!")
  
  log.info("Running PI locally ...")
  import pyspark
  import random
  conf = pyspark.SparkConf()
  conf.setAppName('pi_test')
  conf.setMaster('local[4]')
  conf.set('spark.driver.bindAddress', '127.0.0.1')
  sc = pyspark.SparkContext(conf=conf)
  num_samples = 1000000
  def inside(p):     
    x, y = random.random(), random.random()
    return x*x + y*y < 1
  count = sc.parallelize(range(0, num_samples)).filter(inside).count()
  sc.stop()
  pi = 4 * float(count) / num_samples
  log.info("Pi estimate: %s" % pi)
  assert abs(pi - 3.14) < 0.1, "Spark program had an error?"
  
  log.info("Testing Alluxio-Spark")
  run_cmd('SPARK_LOCAL_IP=127.0.0.1 /opt/alluxio/integration/checker/bin/alluxio-checker.sh spark local[4]')
  

# def alluxio_start_master():
#   run_cmd("""
#     mkdir /mnt/alluxio-ramdisk &&
#     mount -t ramfs -o size=1G ramfs /mnt/alluxio-ramdisk &&
#     chmod a+w /mnt/alluxio-ramdisk
#   """)
#   
#   log.info("ALLUXIO_MASTER_HOSTNAME: %s" % os.environ.get('ALLUXIO_MASTER_HOSTNAME'))
#   
#   MASTER_CMD = '/opt/alluxio/integration/docker/bin/alluxio-master.sh >> /var/log/alluxio-master &'
#   run_cmd(MASTER_CMD)
#    
#   GCS_ACCESS_KEY_ID = os.environ.get('GCS_ACCESS_KEY_ID')
#   GCS_SECRET_ACCESS_KEY = os.environ.get('GCS_SECRET_ACCESS_KEY')
#   assert GCS_ACCESS_KEY_ID
#   run_cmd("""
#     /opt/alluxio/bin/alluxio fs mount
#       --option fs.gcs.accessKeyId=%s
#       --option fs.gcs.secretAccessKey=%s
#       /gcs
#       gs://au2018gs/au2018
#   """ % (GCS_ACCESS_KEY_ID, GCS_SECRET_ACCESS_KEY))
# 
# """
# 
# ALLUXIO_MASTER_HOSTNAME=http://192.168.99.100 /opt/alluxio/bin/alluxio-start.sh local
# /opt/alluxio/bin/alluxio runTests
# /opt/alluxio/integration/fuse/bin/alluxio-fuse mount /mnt/alluxio
# """
# 
# 
# def alluxio_start_worker():
#   run_cmd("""
#     mkdir /mnt/ramdisk &&
#     mount -t ramfs -o size=1G ramfs /mnt/ramdisk &&
#     chmod a+w /mnt/ramdisk
#   """)
#   
#   run_cmd("""
#     mkdir -p /opt/alluxio-fuse &&
#     /opt/alluxio/integration/fuse/bin/alluxio-fuse mount /opt/alluxio-fuse
#   """)
#   
#   run_cmd('/opt/alluxio/integration/docker/bin/alluxio-worker.sh >> /var/log/alluxio-worker &')

def alluxio_start_local():
  log.info("Alluxio really wants a ramdisk, creating ...")
  run_cmd("""
    mkdir -p /mnt/alluxio-ramdisk &&
    (umount -f /mnt/alluxio-ramdisk || true) &&
    mount -t ramfs -o size=2G ramfs /mnt/alluxio-ramdisk &&
    chmod a+w /mnt/alluxio-ramdisk
  """)
  
  log.info("Starting single-node Alluxio local cluster ...")
  run_cmd("/opt/alluxio/bin/alluxio-start.sh local")
  
  log.info("Exposing Alluxio to host via FUSE ...")
  run_cmd("""
    mkdir -p /opt/alluxio-fuse &&
    (umount -f /opt/alluxio-fuse || true) && 
    /opt/alluxio/integration/fuse/bin/alluxio-fuse mount /opt/alluxio-fuse
  """)
  
  GCS_ACCESS_KEY_ID = os.environ.get('GCS_ACCESS_KEY_ID')
  GCS_SECRET_ACCESS_KEY = os.environ.get('GCS_SECRET_ACCESS_KEY')
  AU_GCS_URI = os.environ.get('AU_GCS_URI')
  if AU_GCS_URI and GCS_ACCESS_KEY_ID:
    log.info("Mounting GCS volume ...")
    cmd = """
      /opt/alluxio/bin/alluxio fs mount
        --option fs.gcs.accessKeyId={user}
        --option fs.gcs.secretAccessKey={secret}
        /gcs
        {bucket}
    """.format(user=GCS_ACCESS_KEY_ID, secret=GCS_SECRET_ACCESS_KEY, bucket=AU_GCS_URI)
    run_cmd(cmd)
  
  ALLUXIO_HOST = os.environ.get('ALLUXIO_MASTER_HOSTNAME', 'localhost')
  log.info("""
    Done! Wait a few seconds for startup and then try:
      Alluxio WebUI: http://%s:19999
      Alluxio tests: /opt/alluxio/bin/alluxio runTests
      Alluxio tests only for GCS volume:
        /opt/alluxio/bin/alluxio runTests --directory /gcs
  """ % ALLUXIO_HOST)

## CLI

def create_arg_parser():
  import argparse
  
  parser = argparse.ArgumentParser(
                      description=DESC,
                      formatter_class=argparse.RawDescriptionHelpFormatter)
  parser.add_argument(
    '--shell', default=False, action='store_true',
    help='Drop into a dockerized dev env shell')
  parser.add_argument(
    '--shell-rm', default=False, action='store_true',
    help='Remove the au container')
  parser.add_argument(
    '--build-env', default=False, action='store_true',
    help='Build the dockerized dev env image %s' % DOCKER_IMAGE)
  parser.add_argument(
    '--alluxio-local', default=False, action='store_true',
    help='Start Alluxio local node (e.g. for GFS access & caching)')
  parser.add_argument(
    '--test-spark', default=False, action='store_true',
    help='Ensure Spark works locally')
  
  return parser

def main(args=None):
  if not args:
    parser = create_arg_parser()
    args = parser.parse_args()
  
  if args.build_env:
    DockerEnv.build()
  elif args.shell:
    DockerEnv.shell()
  elif args.shell_rm:
    DockerEnv.rm_shell()
  elif args.alluxio_local:
    alluxio_start_local()
  elif args.test_spark:
    test_spark()

if __name__ == '__main__':
  main()
