#!/usr/bin/env python
# vim: tabstop=2 shiftwidth=2 expandtab

DESC = """
aucli is a tool for boostrapping the AU project.  This tool has limited
dependencies (e.g. just python) so that it can run w/out the source tree.

## Example Workflow
Run all tests in a containerized environment (self-check):
  ./aucli --test-full

Drop into a dockerized development shell:
  ./aucli --shell

Now develop!  Run tests locally:
  pytest au -s --runslow -k test_mnist_save_png

## (Re)-build the dockerized environment image
Outside of the dockerized environment, use:
  ./aucli --build-env

## Tag and publish your Docker images as the latest version:
  ./aucli --push-as-latest

Use the following env vars to override behavior:
  * AU_ROOT - location of AU codebase
  * AU_DOCKER_IMAGE - create a shell or push the au docker image with
      this image name
  * AU_CONTAINER_NAME - create or use a container with this name

"""

import os
import subprocess
import sys

AU_ROOT = os.environ.get('AU_ROOT', os.path.dirname(os.path.abspath(__file__)))
DOCKER_IMAGE_VERSION = 'v1.5.1-draft'
DOCKER_IMAGE = os.environ.get(
  'AU_DOCKER_IMAGE', 'au2018/env:' + DOCKER_IMAGE_VERSION)
CONTAINER_NAME = os.environ.get('AU_CONTAINER_NAME', 'au')

SPARK_WORKER_IMAGE = os.environ.get(
                          'AU_SPARK_WORKER_IMAGE',
                          'au2018/spark:' + DOCKER_IMAGE_VERSION)

## Logging
import logging
LOG_FORMAT = "%(asctime)s\t%(name)-4s %(process)d : %(message)s"
log = logging.getLogger("au")
log.setLevel(logging.INFO)
console_handler = logging.StreamHandler(sys.stderr)
console_handler.setFormatter(logging.Formatter(LOG_FORMAT))
log.addHandler(console_handler)

## Utils

def run_cmd(cmd):
  cmd = cmd.replace('\n', '').strip()
  log.info("Running %s ..." % cmd)
  subprocess.check_call(cmd, shell=True)
  log.info("... done with %s " % cmd)

## Env

class DockerEnv(object):
  
  @classmethod
  def build(cls, image=DOCKER_IMAGE, and_push=True):
    run_cmd(
      'docker build --network=host -t ' + image + ' -f ' + AU_ROOT + '/docker/Dockerfile ' + AU_ROOT)
    if and_push:
      run_cmd('docker push ' + image)

  @classmethod
  def push_as_latest(cls, image=DOCKER_IMAGE):
    latest = image.split(':')[0]
    run_cmd('docker tag ' + image + ' ' + latest)
    run_cmd('docker push ' + latest)

  @classmethod
  def start(cls, container_name=CONTAINER_NAME, image=DOCKER_IMAGE):
    have_nvidia_docker = False
    try:
      run_cmd('nvidia-docker --help > /dev/null')
      have_nvidia_docker = True
    except Exception:
      log.info("Not using nvidia-docker")
    
    env_arg = ''
    if os.path.exists('my.env'):
      env_arg = '--env-file my.env'
    
    # Persist ivy2 / spark jar cache on host.  Saves having to download
    # spark jars every run (can be time consuming on slow connections)
    IVY2_PERSISTED_DIR = '/tmp/au_ivy2'
    run_cmd('mkdir -p %s' % IVY2_PERSISTED_DIR)
    
    docker = 'nvidia-docker' if have_nvidia_docker else 'docker'
    CMD = """
      {docker} run
        --name {container_name}
        -d -it -P
        --net=host
        -v `pwd`:/opt/au:z
        -v {ivy2_persisted_cache}:/root/.ivy2:z
        -v /:/outer_root:z
        -w /opt/au
        {env_arg}
          {docker_image} sleep infinity || docker start {container_name} || true
    """.format(
          docker=docker,
          container_name=container_name,
          docker_image=image,
          env_arg=env_arg,
          ivy2_persisted_cache=IVY2_PERSISTED_DIR)
    run_cmd(CMD)

  @classmethod
  def shell(cls, container_name=CONTAINER_NAME, image=DOCKER_IMAGE):
    cls.start(container_name=container_name, image=image)
    
    # https://github.com/moby/moby/issues/33794#issuecomment-323003975
    #-c "export COLUMNS=`tput cols`; export LINES=`tput lines`; exec bash"'
    EXEC_CMD = 'docker exec -it %s bash' % container_name
    os.execvp("docker", EXEC_CMD.split(' '))

  @classmethod
  def rm_shell(cls, container_name=CONTAINER_NAME):
    try:
      run_cmd('docker rm -f %s' % container_name)
    except Exception:
      pass
    log.info("Removed container %s" % container_name)

  @classmethod
  def run_cmd(cls, cmd, container_name=None, image=None, rm=True):
    
    ### Decide which image to use
    if not image:
      image = DOCKER_IMAGE
    
    have_image = False
    try:
      run_cmd('docker image inspect %s > /dev/null' % image)
      have_image = True
    except Exception:
      pass

    if not have_image:
      log.info("Don't have %s, trying to build ..." % image)
      cls.build(image=image, and_push=False)
      log.info("... done building.")

    log.info("Using docker image %s" % image)


    ### Run `cmd`!
    if not container_name:
      container_name = 'aucli-temp'
    cls.start(container_name=container_name, image=image)
    RUN_CMD = 'docker exec -it %s %s' % (container_name, cmd)
    run_cmd(RUN_CMD)

    if rm:
      cls.rm_shell(container_name=container_name)


## External Projects

def setup_tf_models():
  TF_MODELS_PATH = os.path.join(AU_ROOT, 'external/tensorflow_models/research')
  if not os.path.exists(TF_MODELS_PATH):
    log.warn("Skipping TF Models setup, can't find %s" % TF_MODELS_PATH)
  
  # Do https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md#manual-protobuf-compiler-installation-and-usage
  run_cmd("""
    ls -lhat /opt/au/external/tensorflow_models/research/object_detection/protos/train_pb2.py ||
      ( mkdir -p /opt/protobuf3 ;
        cd /opt/protobuf3 &&
        wget -O protobuf.zip https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip &&
        unzip protobuf.zip &&
        cd - &&
        cd /opt/au/external/tensorflow_models/research &&
        /opt/protobuf3/bin/protoc object_detection/protos/*.proto --python_out=. &&
        python object_detection/builders/model_builder_test.py )
    """)
  
  #run_cmd("""
  #  ls -lhat /tmp/raw-data &&
  #  bash object_detection/dataset_tools/download_and_preprocess_mscoco.sh /tmp
  #""")

def setup_bdd100k():
  assert os.path.exists('/opt/au/cache/data/bdd100k/zips/bdd100k_info.zip'), 'TODO: provide formal copy / audit mechanism'

def setup_mscoco():
  # also need from pycocotools import mask !!!
  setup_tf_models()
  
  # # pycoco tools -- todo fix docker i.e. add protobuf above
  # run_cmd("""
  #   cd /tmp &&
  #   git clone https://github.com/cocodataset/cocoapi.git &&
  #   cd cocoapi/PythonAPI &&
  #   make &&
  #   cp -r pycocotools /opt/au/external/tensorflow_models/research/models/research/
  # """)

  assert os.path.exists('/opt/au/cache/data/mscoco/zips/annotations_trainval2017.zip'), 'TODO: provide formal copy / audit mechanism'

## Services

def test_spark_local():  
  log.info("Do we have Spark installed ? ...")
  import findspark
  findspark.init()
  log.info("... found Spark!")
  
  log.info("Running PI locally ...")
  import pyspark
  import random
  conf = pyspark.SparkConf()
  conf.setAppName('pi_test')
  conf.setMaster('local[4]')
  conf.set('spark.driver.bindAddress', '127.0.0.1')
  sc = pyspark.SparkContext(conf=conf)
  num_samples = 1000000
  def inside(p):     
    x, y = random.random(), random.random()
    return x*x + y*y < 1
  count = sc.parallelize(range(0, num_samples)).filter(inside).count()
  sc.stop()
  pi = 4 * float(count) / num_samples
  log.info("Pi estimate: %s" % pi)
  assert abs(pi - 3.14) < 0.1, "Spark program had an error?"
  
  log.info("Testing Alluxio-Spark")
  run_cmd('SPARK_LOCAL_IP=127.0.0.1 /opt/alluxio/integration/checker/bin/alluxio-checker.sh spark local[4]')
  

# def alluxio_start_master():
#   run_cmd("""
#     mkdir /mnt/alluxio-ramdisk &&
#     mount -t ramfs -o size=1G ramfs /mnt/alluxio-ramdisk &&
#     chmod a+w /mnt/alluxio-ramdisk
#   """)
#   
#   log.info("ALLUXIO_MASTER_HOSTNAME: %s" % os.environ.get('ALLUXIO_MASTER_HOSTNAME'))
#   
#   MASTER_CMD = '/opt/alluxio/integration/docker/bin/alluxio-master.sh >> /var/log/alluxio-master &'
#   run_cmd(MASTER_CMD)
#    
#   GCS_ACCESS_KEY_ID = os.environ.get('GCS_ACCESS_KEY_ID')
#   GCS_SECRET_ACCESS_KEY = os.environ.get('GCS_SECRET_ACCESS_KEY')
#   assert GCS_ACCESS_KEY_ID
#   run_cmd("""
#     /opt/alluxio/bin/alluxio fs mount
#       --option fs.gcs.accessKeyId=%s
#       --option fs.gcs.secretAccessKey=%s
#       /gcs
#       gs://au2018gs/au2018
#   """ % (GCS_ACCESS_KEY_ID, GCS_SECRET_ACCESS_KEY))
# 
# """
# 
# ALLUXIO_MASTER_HOSTNAME=http://192.168.99.100 /opt/alluxio/bin/alluxio-start.sh local
# /opt/alluxio/bin/alluxio runTests
# /opt/alluxio/integration/fuse/bin/alluxio-fuse mount /mnt/alluxio
# """
# 
# 
# def alluxio_start_worker():
#   run_cmd("""
#     mkdir /mnt/ramdisk &&
#     mount -t ramfs -o size=1G ramfs /mnt/ramdisk &&
#     chmod a+w /mnt/ramdisk
#   """)
#   
#   run_cmd("""
#     mkdir -p /opt/alluxio-fuse &&
#     /opt/alluxio/integration/fuse/bin/alluxio-fuse mount /opt/alluxio-fuse
#   """)
#   
#   run_cmd('/opt/alluxio/integration/docker/bin/alluxio-worker.sh >> /var/log/alluxio-worker &')

def alluxio_start_local():
  log.info("Alluxio really wants a ramdisk, creating ...")
  run_cmd("""
    mkdir -p /mnt/alluxio-ramdisk &&
    (umount -f /mnt/alluxio-ramdisk || true) &&
    mount -t ramfs -o size=2G ramfs /mnt/alluxio-ramdisk &&
    chmod a+w /mnt/alluxio-ramdisk
  """)
  
  log.info("Starting single-node Alluxio local cluster ...")
  run_cmd("/opt/alluxio/bin/alluxio-start.sh local")
  
  log.info("Exposing Alluxio to host via FUSE ...")
  run_cmd("""
    mkdir -p /opt/alluxio-fuse &&
    (umount -f /opt/alluxio-fuse || true) && 
    /opt/alluxio/integration/fuse/bin/alluxio-fuse mount /opt/alluxio-fuse
  """)
  
  GCS_ACCESS_KEY_ID = os.environ.get('GCS_ACCESS_KEY_ID')
  GCS_SECRET_ACCESS_KEY = os.environ.get('GCS_SECRET_ACCESS_KEY')
  AU_GCS_URI = os.environ.get('AU_GCS_URI')
  if AU_GCS_URI and GCS_ACCESS_KEY_ID:
    log.info("Mounting GCS volume ...")
    cmd = """
      /opt/alluxio/bin/alluxio fs mount
        --option fs.gcs.accessKeyId={user}
        --option fs.gcs.secretAccessKey={secret}
        /gcs
        {bucket}
    """.format(user=GCS_ACCESS_KEY_ID, secret=GCS_SECRET_ACCESS_KEY, bucket=AU_GCS_URI)
    run_cmd(cmd)
  
  ALLUXIO_HOST = os.environ.get('ALLUXIO_MASTER_HOSTNAME', 'localhost')
  log.info("""
    Done! Wait a few seconds for startup and then try:
      Alluxio WebUI: http://%s:19999
      Alluxio tests: /opt/alluxio/bin/alluxio runTests
      Alluxio tests only for GCS volume:
        /opt/alluxio/bin/alluxio runTests --directory /gcs
  """ % ALLUXIO_HOST)


## K8s-based Cluster

class KubeCluster(object):

  SSH_KEYFILE = os.path.join(AU_ROOT, 'kubespray/cluster_ssh_key')

  HOSTS_INI = os.path.join(AU_ROOT, 'kubespray/inventory/default/hosts.ini')
  HOSTS_INI_TEMPLATE = HOSTS_INI + '.example'

  KUBESPRAY_SRC = os.path.join(AU_ROOT, 'external/kubespray')
  KUBESPRAY_CLUSTER_UP = os.path.join(KUBESPRAY_SRC, 'cluster.yml')

  ARTIFACTS = os.path.join(
                AU_ROOT, 'kubespray/inventory/default/artifacts')
  KUBECTL = os.path.join(ARTIFACTS, 'kubectl')
  ADMIN_CONF = os.path.join(ARTIFACTS, 'admin.conf')

  @classmethod
  def init(cls):
    assert os.path.exists(cls.HOSTS_INI_TEMPLATE), \
      "Are you in an aucli --shell ?"
    
    if not os.path.exists(cls.SSH_KEYFILE):
      print()
      print()
      print("\t\tPlease copy or symlink a cluster SSH private key to ")
      print("\t\t\t" + cls.SSH_KEYFILE)
      print("\t\t(e.g. ln -s /outer_root/home/au/.ssh/your_key_file "  + cls.SSH_KEYFILE + " )")
      print("\t\tand run ./aucli --kube-init again.")
      print()
      sys.exit(1)
    
    if not os.path.exists(cls.HOSTS_INI):
      run_cmd('cp -v ' + cls.HOSTS_INI_TEMPLATE + ' ' + cls.HOSTS_INI)
    
    print()
    print()
    print("\t\tNow please edit " + cls.HOSTS_INI + " to spec your cluster.")
    print("\t\tAfterwards, run $ aucli --kube-up")
    print()
    print()
    sys.exit(0)
  
  @classmethod
  def up(cls):
    assert os.path.exists(cls.KUBESPRAY_CLUSTER_UP), (
      "Do you have submodules pulled?  Use $ git submodule update --init "
      "outside the AU dockerized shell")
    
    cmd = """
      cd {KUBESPRAY_SRC} &&
      ansible-playbook \
        -v --become \
        --key-file {SSH_KEYFILE} \
        -i {HOSTS_INI} \
                cluster.yml 
    """.format(**cls.__dict__)
    run_cmd(cmd)

  @classmethod
  def with_proxy(cls, cmd):
    import time

    pcmd = '{KUBECTL} --kubeconfig {ADMIN_CONF} proxy'.format(**cls.__dict__)
    log.info("Starting proxy: %s" % pcmd)
    p = subprocess.Popen(pcmd.split())
    try:
      run_cmd(cmd)
    finally:
      # This is a mess and may leak zombies.  TODO: find a cleaner way
      # to start the proxy ...
      time.sleep(2)
      log.info("... SIGKILLing proxy ...")
      p.terminate()
      p.kill()
      for _ in range(3):
        time.sleep(1)
        p.poll()
        log.info("... waiting ...")
      log.info("... done.")
    
  @classmethod
  def test(cls):
    assert os.path.exists(cls.ADMIN_CONF), "Did cluster bring-up succeed?"

    print()
    print()
    print("Path to kubectl:")
    print(cls.KUBECTL)
    print()
    print()

    cmd = """
      {KUBECTL} --kubeconfig {ADMIN_CONF} get all --all-namespaces && \
      echo && echo && \
      {KUBECTL} --kubeconfig {ADMIN_CONF} cluster-info
    """.format(**cls.__dict__)
    run_cmd(cmd)

    test_spark = "python -c 'from au.spark import K8SSpark; K8SSpark.selftest()'"
    cls.with_proxy(test_spark)

  @classmethod
  def build_spark_env(cls, image=SPARK_WORKER_IMAGE, and_push=True):
    run_cmd(
      'cd ' + os.path.join(AU_ROOT, 'cluster/spark') + ' && '
      'docker build --network=host  --build-arg base_img=' + DOCKER_IMAGE + ' -t ' + image + ' .')
    if and_push:
      run_cmd('docker push ' + image)
  
  @classmethod
  def push_as_latest(cls, image=SPARK_WORKER_IMAGE):
    cls.build_spark_env(image=image)
    latest = image.split(':')[0]
    run_cmd('docker tag ' + image + ' ' + latest)
    run_cmd('docker push ' + latest)

## CLI

def create_arg_parser():
  import argparse
  
  parser = argparse.ArgumentParser(
                      description=DESC,
                      formatter_class=argparse.RawDescriptionHelpFormatter)
  parser.add_argument(
    '--shell', default=False, action='store_true',
    help='Drop into a dockerized dev env shell')
  parser.add_argument(
    '--shell-rm', default=False, action='store_true',
    help='Remove the au container')
  parser.add_argument(
    '--build-env', default=False, action='store_true',
    help='Build the dockerized dev env image %s' % DOCKER_IMAGE)
  parser.add_argument(
    '--push-as-latest', default=False, action='store_true',
    help='Tag Docker images at latest and push them')

  parser.add_argument(
    '--alluxio-local', default=False, action='store_true',
    help='Start Alluxio local node (e.g. for GFS access & caching)')
  
  parser.add_argument(
    '--test-spark', default=False, action='store_true',
    help='Ensure Spark works locally')
  parser.add_argument(
    '--test-full', default=False, action='store_true',
    help='Run the full set of tests with the local source tree in a '
         'containerized environment')
  
  parser.add_argument(
    '--kube-init', default=False, action='store_true',
    help='Prepare kubespray cluster launch')
  parser.add_argument(
    '--kube-up', default=False, action='store_true',
    help='Bring up the k8s cluster via Kubespray')
  parser.add_argument(
    '--kube-test', default=False, action='store_true',
    help='Show k8s pods and provide kubectl usage')

  parser.add_argument(
    '--setup-external', default=False, action='store_true',
    help='Set up external projects and datasets')

  return parser

def main(args=None):
  if not args:
    parser = create_arg_parser()
    args = parser.parse_args()
  
  if args.build_env:
    DockerEnv.build()
    KubeCluster.build_spark_env()
  elif args.shell:
    DockerEnv.shell()
  elif args.shell_rm:
    DockerEnv.rm_shell()
  elif args.push_as_latest:
    DockerEnv.push_as_latest()
    KubeCluster.push_as_latest()
  
  elif args.alluxio_local:
    alluxio_start_local()
  
  elif args.test_spark:
    test_spark_local()
  elif args.test_full:
    DockerEnv.run_cmd(
      'pytest au -s --runslow',
      container_name='aucli_full_test')
  
  elif args.kube_init:
    KubeCluster.init()
  elif args.kube_up:
    KubeCluster.up()
  elif args.kube_test:
    KubeCluster.test()

  elif args.setup_external:
    setup_tf_models()
    # setup_bdd100k()

if __name__ == '__main__':
  main()
